{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Moussou_Lab2_JoeyNMT_Advanced.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRwdfvuQ486V"
      },
      "source": [
        "# Lab 2: Joey NMT Advanced\n",
        "\n",
        "In this notebook, we'll train a Transformer model for translating between TED talks in French (*fr*) and English (*en*). We'll do some configuration debugging and experiment with hyperparameters, then inspect evaluation metrics and find out how robust the model is.\n",
        "\n",
        "The pre-processing code is a bit lengthy, but it reflects reality: often getting the data into the right format and selecting the right pieces takes more code than the actual model training ;) \n",
        "\n",
        "At the very end of this colab you'll also find instructions on how to get started with backtranslation as a data augmentation technique, and how to build a multilingual model. These topics are not mandatory but might be fun to explore if you have time. \n",
        "\n",
        "**Important:** Before you start, set runtime type to GPU.\n",
        "\n",
        "Author: Julia Kreutzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx1neLQt3vZ0"
      },
      "source": [
        "import os"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t24FKds4_oj",
        "outputId": "6c514491-eb69-4246-a242-7069a86cd22a"
      },
      "source": [
        "!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.8.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_-Qu5xo5ILb",
        "outputId": "2d9f56b9-ac51-48c5-e9b0-989e986274ef"
      },
      "source": [
        "!pip install joeynmt"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: joeynmt in /usr/local/lib/python3.7/dist-packages (1.3)\n",
            "Requirement already satisfied: numpy==1.20.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.20.1)\n",
            "Requirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.9.3)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.8.0+cu101)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.1.2)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.11.1)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.3.7)\n",
            "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (57.0.0)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (5.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n",
            "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.5.1)\n",
            "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.9.0)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.6.1)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (5.9.2)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n",
            "Requirement already satisfied: astroid<2.7,>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (2.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt) (3.7.4.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.31.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (4.41.1)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.2->pylint->joeynmt) (1.6.0)\n",
            "Requirement already satisfied: typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.2->pylint->joeynmt) (1.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNHhMgH55P8l"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "We'll use *English - French* translations from the [IWSLT 2017 challenge](https://wit3.fbk.eu/2017-01-c), the [\"unofficial\" task](https://sites.google.com/site/iwsltevaluation2017/TED-tasks?authuser=0). This challenge is about translating TED talks from multiple languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KrEuHrz5dEt"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTCwLsM5_He"
      },
      "source": [
        "Requires downloading a file of 292MB. If you do this ahead of time, store a copy of it in your Google drive and access it from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qUEf7DX5whA",
        "outputId": "532e7b4d-4ae5-41eb-b771-894e1a562dd0"
      },
      "source": [
        "! pip install gdown"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3B9ejpL5LFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b744bb-3293-4338-f2c5-a7c2673802fd"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2\n",
            "To: /content/2017-01-trnted.tgz\n",
            "292MB [00:04, 67.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJuXNbBA5ly6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b9b397-3212-45b7-ea1e-682c38478495"
      },
      "source": [
        "! tar -zxvf /content/2017-01-trnted.tgz"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2017-01-trnted/\n",
            "2017-01-trnted/texts/\n",
            "2017-01-trnted/._texts.html\n",
            "2017-01-trnted/texts.html\n",
            "2017-01-trnted/texts/ar/\n",
            "2017-01-trnted/texts/de/\n",
            "2017-01-trnted/texts/en/\n",
            "2017-01-trnted/texts/fr/\n",
            "2017-01-trnted/texts/ja/\n",
            "2017-01-trnted/texts/ko/\n",
            "2017-01-trnted/texts/zh/\n",
            "2017-01-trnted/texts/zh/en/\n",
            "2017-01-trnted/texts/zh/en/._.eval\n",
            "2017-01-trnted/texts/zh/en/.eval\n",
            "2017-01-trnted/texts/zh/en/._.info\n",
            "2017-01-trnted/texts/zh/en/.info\n",
            "2017-01-trnted/texts/zh/en/._zh-en.tgz\n",
            "2017-01-trnted/texts/zh/en/zh-en.tgz\n",
            "2017-01-trnted/texts/ko/en/\n",
            "2017-01-trnted/texts/ko/en/._.eval\n",
            "2017-01-trnted/texts/ko/en/.eval\n",
            "2017-01-trnted/texts/ko/en/._.info\n",
            "2017-01-trnted/texts/ko/en/.info\n",
            "2017-01-trnted/texts/ko/en/._ko-en.tgz\n",
            "2017-01-trnted/texts/ko/en/ko-en.tgz\n",
            "2017-01-trnted/texts/ja/en/\n",
            "2017-01-trnted/texts/ja/en/._.eval\n",
            "2017-01-trnted/texts/ja/en/.eval\n",
            "2017-01-trnted/texts/ja/en/._.info\n",
            "2017-01-trnted/texts/ja/en/.info\n",
            "2017-01-trnted/texts/ja/en/._ja-en.tgz\n",
            "2017-01-trnted/texts/ja/en/ja-en.tgz\n",
            "2017-01-trnted/texts/fr/en/\n",
            "2017-01-trnted/texts/fr/en/._.eval\n",
            "2017-01-trnted/texts/fr/en/.eval\n",
            "2017-01-trnted/texts/fr/en/._.info\n",
            "2017-01-trnted/texts/fr/en/.info\n",
            "2017-01-trnted/texts/fr/en/._fr-en.tgz\n",
            "2017-01-trnted/texts/fr/en/fr-en.tgz\n",
            "2017-01-trnted/texts/en/ar/\n",
            "2017-01-trnted/texts/en/de/\n",
            "2017-01-trnted/texts/en/fr/\n",
            "2017-01-trnted/texts/en/ja/\n",
            "2017-01-trnted/texts/en/ko/\n",
            "2017-01-trnted/texts/en/zh/\n",
            "2017-01-trnted/texts/en/zh/._.eval\n",
            "2017-01-trnted/texts/en/zh/.eval\n",
            "2017-01-trnted/texts/en/zh/._.info\n",
            "2017-01-trnted/texts/en/zh/.info\n",
            "2017-01-trnted/texts/en/zh/._en-zh.tgz\n",
            "2017-01-trnted/texts/en/zh/en-zh.tgz\n",
            "2017-01-trnted/texts/en/ko/._.eval\n",
            "2017-01-trnted/texts/en/ko/.eval\n",
            "2017-01-trnted/texts/en/ko/._.info\n",
            "2017-01-trnted/texts/en/ko/.info\n",
            "2017-01-trnted/texts/en/ko/._en-ko.tgz\n",
            "2017-01-trnted/texts/en/ko/en-ko.tgz\n",
            "2017-01-trnted/texts/en/ja/._.eval\n",
            "2017-01-trnted/texts/en/ja/.eval\n",
            "2017-01-trnted/texts/en/ja/._.info\n",
            "2017-01-trnted/texts/en/ja/.info\n",
            "2017-01-trnted/texts/en/ja/._en-ja.tgz\n",
            "2017-01-trnted/texts/en/ja/en-ja.tgz\n",
            "2017-01-trnted/texts/en/fr/._.eval\n",
            "2017-01-trnted/texts/en/fr/.eval\n",
            "2017-01-trnted/texts/en/fr/._.info\n",
            "2017-01-trnted/texts/en/fr/.info\n",
            "2017-01-trnted/texts/en/fr/._en-fr.tgz\n",
            "2017-01-trnted/texts/en/fr/en-fr.tgz\n",
            "2017-01-trnted/texts/en/de/._.eval\n",
            "2017-01-trnted/texts/en/de/.eval\n",
            "2017-01-trnted/texts/en/de/._.info\n",
            "2017-01-trnted/texts/en/de/.info\n",
            "2017-01-trnted/texts/en/de/._en-de.tgz\n",
            "2017-01-trnted/texts/en/de/en-de.tgz\n",
            "2017-01-trnted/texts/en/ar/._.eval\n",
            "2017-01-trnted/texts/en/ar/.eval\n",
            "2017-01-trnted/texts/en/ar/._.info\n",
            "2017-01-trnted/texts/en/ar/.info\n",
            "2017-01-trnted/texts/en/ar/._en-ar.tgz\n",
            "2017-01-trnted/texts/en/ar/en-ar.tgz\n",
            "2017-01-trnted/texts/de/en/\n",
            "2017-01-trnted/texts/de/en/._.eval\n",
            "2017-01-trnted/texts/de/en/.eval\n",
            "2017-01-trnted/texts/de/en/._.info\n",
            "2017-01-trnted/texts/de/en/.info\n",
            "2017-01-trnted/texts/de/en/._de-en.tgz\n",
            "2017-01-trnted/texts/de/en/de-en.tgz\n",
            "2017-01-trnted/texts/ar/en/\n",
            "2017-01-trnted/texts/ar/en/._.eval\n",
            "2017-01-trnted/texts/ar/en/.eval\n",
            "2017-01-trnted/texts/ar/en/._.info\n",
            "2017-01-trnted/texts/ar/en/.info\n",
            "2017-01-trnted/texts/ar/en/._ar-en.tgz\n",
            "2017-01-trnted/texts/ar/en/ar-en.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrN51fJ6cjJ"
      },
      "source": [
        "The `texts` subdirectory contains translation data for multiple languages. Let's start with `fr-en`, French to English translations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAVpplSI6YW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa4590f-7b01-40ee-cc40-6f9fcb899807"
      },
      "source": [
        "!tar -xvf 2017-01-trnted/texts/fr/en/fr-en.tgz"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fr-en/\n",
            "fr-en/IWSLT17.TED.dev2010.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.dev2010.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2010.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2010.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2011.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2011.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2012.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2012.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2013.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2013.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2014.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml\n",
            "fr-en/IWSLT17.TED.tst2015.fr-en.en.xml\n",
            "fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml\n",
            "fr-en/README\n",
            "fr-en/train.en\n",
            "fr-en/train.tags.fr-en.en\n",
            "fr-en/train.tags.fr-en.fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pNHfnZHnzb"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGjjucDo7ZtO"
      },
      "source": [
        "The parallel data is stored in XML, see the description in the README. But it's multiple documents per file, so XML parsing requires splitting it. We'll go the quick and dirty way, as in this [pre-processing script](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh) by just removing all metainformation that we're not interested in (i.e. every line containing a html tag. This is *not a good example for mindful pre-processing*, but good enough for now. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjktf_ky6qgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3398f0-46a1-4cb6-867a-9414ddb2d833"
      },
      "source": [
        "! head -n 20 /content/fr-en/train.tags.fr-en.en"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<doc docid=\"1\" genre=\"lectures\"> \n",
            "<url>http://www.ted.com/talks/al_gore_on_averting_climate_crisis</url> \n",
            "<keywords>talks, alternative energy, cars, climate change, culture, environment, global issues, politics, science, sustainability, technology</keywords> \n",
            "<speaker>Al Gore</speaker> \n",
            "<talkid>1</talkid> \n",
            "<title>Al Gore: Averting the climate crisis</title> \n",
            "<description>TED Talk Subtitles and Transcript: With the same humor and humanity he exuded in \"An Inconvenient Truth,\" Al Gore spells out 15 ways that individuals can address climate change immediately, from buying a hybrid to inventing a new, hotter brand name for global warming.</description> \n",
            "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. \n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. \n",
            "And I say that sincerely, partly because  Put yourselves in my position. \n",
            "I flew on Air Force Two for eight years. \n",
            "Now I have to take off my shoes or boots to get on an airplane! \n",
            " I'll tell you one quick story to illustrate what that's been like for me. \n",
            "It's a true story -- every bit of this is true. \n",
            "Soon after Tipper and I left the --  White House --  we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville. \n",
            "Driving ourselves. \n",
            "I know it sounds like a little thing to you, but -- I looked in the rear-view mirror and all of a sudden it just hit me. \n",
            "There was no motorcade back there. \n",
            "You've heard of phantom limb pain?  This was a rented Ford Taurus.  It was dinnertime, and we started looking for a place to eat. \n",
            "We were on I-40. We got to Exit 238, Lebanon, Tennessee. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhXOSyKVBRae"
      },
      "source": [
        "def remove_xml(filename):\n",
        "  \"\"\"Remove all lines that contain xml brackets except for those in <seg>.\"\"\"\n",
        "  valid_lines = []\n",
        "  with open(filename, 'r') as ofile:\n",
        "    for line in ofile:\n",
        "      if ('<' in line or '>' in line) and not '<seg' in line:\n",
        "        continue\n",
        "      else:\n",
        "        # Get content between <seg> tags for dev and test sets.\n",
        "        if '<seg' in line:\n",
        "          content = line.strip().split('>')[1].split('<')[0]\n",
        "        else: \n",
        "          content = line.strip()\n",
        "        valid_lines.append(content)\n",
        "  return valid_lines"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N5gyBpz8jeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bc17b3-0405-41b0-8e44-0b0d9e641b5d"
      },
      "source": [
        "targets = remove_xml('/content/fr-en/train.tags.fr-en.en')\n",
        "print(f'Read {len(targets)} target sentences.')\n",
        "    \n",
        "sources = remove_xml('/content/fr-en/train.tags.fr-en.fr')\n",
        "print(f'Read {len(sources)} source sentences.')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 232825 target sentences.\n",
            "Read 232825 source sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNoL2WyF_rgD"
      },
      "source": [
        "Let's check if they match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFgKyecW8nSP",
        "outputId": "046a7a8e-975c-4f3a-f7b9-92a1735fe427"
      },
      "source": [
        "num_examples = 3\n",
        "for s, t in zip(sources[:num_examples], targets[:num_examples]):\n",
        "  print(s)\n",
        "  print(t)\n",
        "  print()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\n",
            "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "\n",
            "J'ai été très impressionné par cette conférence, et je tiens à vous remercier tous pour vos nombreux et sympathiques commentaires sur ce que j'ai dit l'autre soir.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "\n",
            "Et je dis çà sincèrement, en autres parce que --Faux sanglot-- j'en ai besoin ! --Rires-- Mettez-vous à ma place!\n",
            "And I say that sincerely, partly because  Put yourselves in my position.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iais2WO6AIa5"
      },
      "source": [
        "Looks good! You might already see that the translations are sometimes not very literal. Let's write them into file to feed them to Joey NMT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DwIq9Su_2Pl"
      },
      "source": [
        "def write_to_file(sentences, filename):\n",
        "  \"\"\"Write sentences to file.\"\"\"\n",
        "  with open(filename, 'w') as ofile:\n",
        "    for sent in sentences:\n",
        "      ofile.write(sent.strip()+'\\n')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B7ZLZo2KyiS"
      },
      "source": [
        "data_dir = '/content/fr-en'"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ-aWXlGHCvv"
      },
      "source": [
        "file_prefix = 'parallel_'"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0rTleLHI5k"
      },
      "source": [
        "src_lang = 'fr'\n",
        "trg_lang = 'en'"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve2BE5mcBPuO"
      },
      "source": [
        "train_src_file = os.path.join(data_dir, file_prefix+'train.'+src_lang)\n",
        "train_trg_file = os.path.join(data_dir, file_prefix+'train.'+trg_lang)\n",
        "write_to_file(targets, train_trg_file)\n",
        "write_to_file(sources, train_src_file)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IckyC5M9Cv3-"
      },
      "source": [
        "Great, now we need a development and a test set. As development set we can pick any of the `tst` or `dev` files in the data we just downloaded (these were used for testing and evaluation for previous years). We'll go with `tst2015` for testing and `tst2014` for development."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR2g1vWgFQLy"
      },
      "source": [
        "**Question for you**: Is this choice important? How do you think selecting a different dev/test set could influence our findings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO_zFPcbFOHT",
        "outputId": "2ef0ee7f-ce88-40a5-8e43-b4381fbf4790"
      },
      "source": [
        "test_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.en.xml')\n",
        "print(f'Read {len(test_targets)} test target sentences.')\n",
        "    \n",
        "test_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml')\n",
        "print(f'Read {len(test_sources)} test source sentences.')\n",
        "\n",
        "dev_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.en.xml')\n",
        "print(f'Read {len(dev_targets)} dev target sentences.')\n",
        "    \n",
        "dev_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml')\n",
        "print(f'Read {len(dev_sources)} dev source sentences.')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1210 test target sentences.\n",
            "Read 1210 test source sentences.\n",
            "Read 1306 dev target sentences.\n",
            "Read 1306 dev source sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_UAV5oiFpmu"
      },
      "source": [
        "dev_src_file = os.path.join(data_dir, file_prefix+'dev.'+src_lang)\n",
        "dev_trg_file = os.path.join(data_dir, file_prefix+'dev.'+trg_lang)\n",
        "test_src_file = os.path.join(data_dir, file_prefix+'test.'+src_lang)\n",
        "test_trg_file = os.path.join(data_dir, file_prefix+'test.'+trg_lang)\n",
        "\n",
        "write_to_file(dev_targets, dev_trg_file)\n",
        "write_to_file(dev_sources, dev_src_file)\n",
        "write_to_file(test_targets, test_trg_file)\n",
        "write_to_file(test_sources, test_src_file)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOClab0XHhvh"
      },
      "source": [
        "## Sub-words\n",
        "\n",
        "Same procedure as in Lab 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R_1HRBvHBn-"
      },
      "source": [
        "bpe_size = 4000"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDxhoKCuKnPE"
      },
      "source": [
        "train_joint_file = os.path.join(data_dir, file_prefix+'train.'+src_lang+'-'+trg_lang)\n",
        "\n",
        "src_files = {'train': train_src_file, 'dev': dev_src_file, 'test': test_src_file}\n",
        "trg_files = {'train': train_trg_file, 'dev': dev_trg_file, 'test': test_trg_file}\n",
        "\n",
        "vocab_src_file = os.path.join(data_dir, f'vocab.{bpe_size}.{src_lang}')\n",
        "vocab_trg_file = os.path.join(data_dir, f'vocab.{bpe_size}.{trg_lang}')\n",
        "bpe_file = os.path.join(data_dir, f'bpe.codes.{bpe_size}')"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9OpCuhsKpIZ"
      },
      "source": [
        "! cat $train_src_file $train_trg_file > $train_joint_file\n",
        "\n",
        "! subword-nmt learn-bpe \\\n",
        "  --input $train_joint_file \\\n",
        "  -s $bpe_size \\\n",
        "  -o $bpe_file"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZiHb1iVKsWS"
      },
      "source": [
        "src_bpe_files = {}\n",
        "trg_bpe_files = {}\n",
        "for split in ['train', 'dev', 'test']:\n",
        "  src_input_file = src_files[split]\n",
        "  trg_input_file = trg_files[split]\n",
        "  src_output_file = src_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  trg_output_file = trg_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n",
        "  src_bpe_files[split] = src_output_file\n",
        "  trg_bpe_files[split] = trg_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $src_input_file > $src_output_file\n",
        "\n",
        "  ! subword-nmt apply-bpe \\\n",
        "    -c $bpe_file \\\n",
        "    < $trg_input_file > $trg_output_file\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3TMw58tKu5M",
        "outputId": "5207df1d-8865-4958-ce28-380e751f2dca"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 10:17:49--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2034 (2.0K) [text/plain]\n",
            "Saving to: ‘build_vocab.py.1’\n",
            "\n",
            "\rbuild_vocab.py.1      0%[                    ]       0  --.-KB/s               \rbuild_vocab.py.1    100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-08 10:17:49 (34.6 MB/s) - ‘build_vocab.py.1’ saved [2034/2034]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IzclnElKwkg"
      },
      "source": [
        "vocab_src_file = src_bpe_files['train']\n",
        "vocab_trg_file = trg_bpe_files['train']\n",
        "bpe_vocab_file = os.path.join(data_dir, f'joint.{bpe_size}bpe.vocab')\n",
        "\n",
        "! python build_vocab.py  \\\n",
        "  $vocab_src_file $vocab_trg_file \\\n",
        "  --output_path $bpe_vocab_file"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U92TAIbLO0vV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOswhM_cTkVu",
        "outputId": "62ebaa16-0e1e-4dc1-d6c5-affdfadd595d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive_home = '/content/drive'\n",
        "drive.mount(drive_home)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoiHXv_STskk"
      },
      "source": [
        "g_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/models/%s-%s\" % (src_lang, trg_lang)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfX4_9cGTXHL"
      },
      "source": [
        "experiment_name = 'ted_fr_en'"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJh0MywhTzhq"
      },
      "source": [
        "model_path = os.path.join(g_drive_path, experiment_name)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLm8TR2LFbVX"
      },
      "source": [
        "Copy the BPE merges to Gdrive so we don't lose them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoLrEKP9FY5e"
      },
      "source": [
        "bpe_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/bpe/%s-%s\" % (src_lang, trg_lang)\n",
        "! mkdir -p $bpe_drive_path\n",
        "\n",
        "! cp $bpe_file $bpe_drive_path "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvYBPK2nO2us"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "The following configuration file contains *three* bugs that prevent it from working (=quickly giving reasonable BLEU for translating between French and English). Find and fix those three. Try not to compare with the config from Lab 1 ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG6ftq_2PVIu"
      },
      "source": [
        "# Create the config\n",
        "broken_config = \"\"\"\n",
        "name: \"{name}\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "\n",
        "    train: \"{data_dir}/parallel_train.{bpe_size}.bpe\"  \n",
        "    dev:   \"{data_dir}/parallel_dev.{bpe_size}.bpe\"\n",
        "    test:  \"{data_dir}/parallel_test.{bpe_size}.bpe\"\n",
        "    level: \"bpe\"                   # Here we specify we're working on BPEs.\n",
        "    lowercase: False                \n",
        "    max_sent_length: 30             # Extend to longer sentences.\n",
        "    src_vocab: \"{src_vocab_path}\"\n",
        "    trg_vocab: \"{trg_vocab_path}\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"intl\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{model_path}/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # Alternative: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 3                  # Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 500          # Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"{model_path}\"\n",
        "    overwrite: True                # Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True        # Joint vocabulary.\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # Increase to 512 for larger data.\n",
        "        ff_size: 1024            # Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=experiment_name, \n",
        "           source_language=src_lang, \n",
        "           target_language=trg_lang,\n",
        "           data_dir=data_dir, \n",
        "           model_path=model_path, \n",
        "           src_vocab_path=bpe_vocab_file,\n",
        "           trg_vocab_path=bpe_vocab_file, \n",
        "           bpe_size=bpe_size)\n",
        "with open(\"transformer_{name}.yaml\".format(name=experiment_name),'w') as f:\n",
        "    f.write(broken_config)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF14SqV0UbfM"
      },
      "source": [
        "If you try running this training multiple times for debugging, set `overwrite` to `True` in the config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNILzUckUFFv",
        "outputId": "afd1e331-e960-4d00-aa5b-e51e8f02992a"
      },
      "source": [
        "!python -m joeynmt train transformer_ted_fr_en.yaml"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:17:55,232 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-08 10:17:55,262 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-08 10:17:59,667 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-08 10:17:59,945 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-08 10:17:59,964 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-08 10:17:59,984 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-08 10:17:59,984 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:18:00,180 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-08 10:18:00.358345: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-08 10:18:01,839 - INFO - joeynmt.training - Total params: 12170752\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.name                           : ted_fr_en\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.src                       : fr\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.train                     : /content/fr-en/parallel_train.4000.bpe\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/fr-en/parallel_dev.4000.bpe\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.test                      : /content/fr-en/parallel_test.4000.bpe\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 30\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/fr-en/joint.4000bpe.vocab\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/fr-en/joint.4000bpe.vocab\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-08 10:18:05,230 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : intl\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-08 10:18:05,231 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.epochs                : 3\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 500\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.model_dir             : /content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-08 10:18:05,232 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-08 10:18:05,233 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 148618,\n",
            "\tvalid 1306,\n",
            "\ttest 1210\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] J'ai vol@@ é avec A@@ ir For@@ ce 2 pendant hu@@ it ans.\n",
            "\t[TRG] I f@@ le@@ w on A@@ ir For@@ ce T@@ w@@ o for ei@@ ght years.\n",
            "2021-07-08 10:18:05,234 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) a (7) to (8) of (9) and\n",
            "2021-07-08 10:18:05,235 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) a (7) to (8) of (9) and\n",
            "2021-07-08 10:18:05,235 - INFO - joeynmt.helpers - Number of Src words (types): 4338\n",
            "2021-07-08 10:18:05,235 - INFO - joeynmt.helpers - Number of Trg words (types): 4338\n",
            "2021-07-08 10:18:05,235 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4338),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4338))\n",
            "2021-07-08 10:18:05,239 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-08 10:18:05,239 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-08 10:18:19,105 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.801359, Tokens per Sec:    18213, Lr: 0.000300\n",
            "2021-07-08 10:18:32,825 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.684990, Tokens per Sec:    18483, Lr: 0.000300\n",
            "2021-07-08 10:18:46,783 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.932720, Tokens per Sec:    18381, Lr: 0.000300\n",
            "2021-07-08 10:19:00,818 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.014529, Tokens per Sec:    18116, Lr: 0.000300\n",
            "2021-07-08 10:19:14,973 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.850861, Tokens per Sec:    17988, Lr: 0.000300\n",
            "2021-07-08 10:20:17,101 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-08 10:20:17,101 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-08 10:20:17,509 - INFO - joeynmt.training - Example #0\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tHypothesis: And I I I I I have the of the of the of the time.\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - Example #1\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tReference:  Not good enough.\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tHypothesis: And you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - Example #2\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n",
            "2021-07-08 10:20:17,510 - INFO - joeynmt.training - \tHypothesis: And I I I I have to the of the little of the of the little of the time.\n",
            "2021-07-08 10:20:17,511 - INFO - joeynmt.training - Example #3\n",
            "2021-07-08 10:20:17,511 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n",
            "2021-07-08 10:20:17,511 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n",
            "2021-07-08 10:20:17,511 - INFO - joeynmt.training - \tHypothesis: And I I I I I I I I we we we we we we we we we we we we we we have the the the the the lot of the the the the lot of the lot of the lot of the the the the the lot of the seed of the sed of the seed the seed of the lot of the seed the seed of the the the lot of the lot of the the the the the the lot of the lot of the\n",
            "2021-07-08 10:20:17,511 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      500: bleu:   0.05, loss: 178315.9688, ppl: 183.7007, duration: 62.5381s\n",
            "2021-07-08 10:20:32,499 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.930799, Tokens per Sec:    17204, Lr: 0.000300\n",
            "2021-07-08 10:20:47,325 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.788768, Tokens per Sec:    17240, Lr: 0.000300\n",
            "2021-07-08 10:21:02,064 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.746156, Tokens per Sec:    17243, Lr: 0.000300\n",
            "2021-07-08 10:21:16,770 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.721320, Tokens per Sec:    17316, Lr: 0.000300\n",
            "2021-07-08 10:21:26,312 - INFO - joeynmt.training - Epoch   1: total training loss 4866.40\n",
            "2021-07-08 10:21:26,312 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-08 10:21:31,690 - INFO - joeynmt.training - Epoch   2, Step:     1000, Batch Loss:     4.441039, Tokens per Sec:    16650, Lr: 0.000300\n",
            "2021-07-08 10:22:30,148 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-08 10:22:30,148 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - Example #0\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tHypothesis: This is a very very very very very very very very very much.\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - Example #1\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tReference:  Not good enough.\n",
            "2021-07-08 10:22:30,553 - INFO - joeynmt.training - \tHypothesis: It's not not not not not not not not not not not not not not not not not it.\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - Example #2\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tHypothesis: S: And the M: I said, \"What is a very very very much.\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - Example #3\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - \tHypothesis: I was a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very\n",
            "2021-07-08 10:22:30,554 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1000: bleu:   1.24, loss: 166074.3438, ppl: 128.4330, duration: 58.8636s\n",
            "2021-07-08 10:22:45,063 - INFO - joeynmt.training - Epoch   2, Step:     1100, Batch Loss:     4.699810, Tokens per Sec:    17137, Lr: 0.000300\n",
            "2021-07-08 10:22:59,761 - INFO - joeynmt.training - Epoch   2, Step:     1200, Batch Loss:     4.426465, Tokens per Sec:    17558, Lr: 0.000300\n",
            "2021-07-08 10:23:14,455 - INFO - joeynmt.training - Epoch   2, Step:     1300, Batch Loss:     4.347055, Tokens per Sec:    17305, Lr: 0.000300\n",
            "2021-07-08 10:23:28,989 - INFO - joeynmt.training - Epoch   2, Step:     1400, Batch Loss:     4.310916, Tokens per Sec:    17269, Lr: 0.000300\n",
            "2021-07-08 10:23:43,563 - INFO - joeynmt.training - Epoch   2, Step:     1500, Batch Loss:     2.043632, Tokens per Sec:    17380, Lr: 0.000300\n",
            "2021-07-08 10:24:46,033 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-08 10:24:46,033 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-08 10:24:46,460 - INFO - joeynmt.training - Example #0\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tHypothesis: SM: Who is what is what is what is it?\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - Example #1\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tReference:  Not good enough.\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tHypothesis: It's not going to be to be much.\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - Example #2\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n",
            "2021-07-08 10:24:46,461 - INFO - joeynmt.training - \tHypothesis: SM: Who is what do you do that?\n",
            "2021-07-08 10:24:46,462 - INFO - joeynmt.training - Example #3\n",
            "2021-07-08 10:24:46,462 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n",
            "2021-07-08 10:24:46,462 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n",
            "2021-07-08 10:24:46,462 - INFO - joeynmt.training - \tHypothesis: I was a Bik: I don't have a SSSSSSSia, and I don't know it was the same thing.\n",
            "2021-07-08 10:24:46,462 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1500: bleu:   1.97, loss: 151583.5781, ppl:  84.0785, duration: 62.8982s\n",
            "2021-07-08 10:25:01,168 - INFO - joeynmt.training - Epoch   2, Step:     1600, Batch Loss:     4.189253, Tokens per Sec:    17622, Lr: 0.000300\n",
            "2021-07-08 10:25:15,637 - INFO - joeynmt.training - Epoch   2, Step:     1700, Batch Loss:     4.153015, Tokens per Sec:    17405, Lr: 0.000300\n",
            "2021-07-08 10:25:30,214 - INFO - joeynmt.training - Epoch   2, Step:     1800, Batch Loss:     4.099439, Tokens per Sec:    17374, Lr: 0.000300\n",
            "2021-07-08 10:25:44,949 - INFO - joeynmt.training - Epoch   2, Step:     1900, Batch Loss:     4.073881, Tokens per Sec:    17477, Lr: 0.000300\n",
            "2021-07-08 10:25:49,752 - INFO - joeynmt.training - Epoch   2: total training loss 3952.64\n",
            "2021-07-08 10:25:49,752 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-08 10:25:59,717 - INFO - joeynmt.training - Epoch   3, Step:     2000, Batch Loss:     3.566428, Tokens per Sec:    17116, Lr: 0.000300\n",
            "2021-07-08 10:26:49,531 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-08 10:26:49,532 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - Example #0\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tHypothesis: Can the M: What do you do that?\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - Example #1\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tReference:  Not good enough.\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - \tHypothesis: It's not going to be there.\n",
            "2021-07-08 10:26:49,935 - INFO - joeynmt.training - Example #2\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tHypothesis: Can M: How do you do that?\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - Example #3\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - \tHypothesis: I was called \"No, and I don't have a Sinh and I don't know that I was the same thing.\n",
            "2021-07-08 10:26:49,936 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2000: bleu:   3.71, loss: 141578.1406, ppl:  62.7543, duration: 50.2190s\n",
            "2021-07-08 10:27:04,609 - INFO - joeynmt.training - Epoch   3, Step:     2100, Batch Loss:     3.717963, Tokens per Sec:    17381, Lr: 0.000300\n",
            "2021-07-08 10:27:19,318 - INFO - joeynmt.training - Epoch   3, Step:     2200, Batch Loss:     3.872044, Tokens per Sec:    17590, Lr: 0.000300\n",
            "2021-07-08 10:27:33,966 - INFO - joeynmt.training - Epoch   3, Step:     2300, Batch Loss:     3.726541, Tokens per Sec:    17221, Lr: 0.000300\n",
            "2021-07-08 10:27:48,435 - INFO - joeynmt.training - Epoch   3, Step:     2400, Batch Loss:     3.277330, Tokens per Sec:    17324, Lr: 0.000300\n",
            "2021-07-08 10:28:03,075 - INFO - joeynmt.training - Epoch   3, Step:     2500, Batch Loss:     3.370359, Tokens per Sec:    17228, Lr: 0.000300\n",
            "2021-07-08 10:29:05,406 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-08 10:29:05,406 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - Example #0\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tHypothesis: Would you call this God -- right?\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - Example #1\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tReference:  Not good enough.\n",
            "2021-07-08 10:29:05,799 - INFO - joeynmt.training - \tHypothesis: It's not just that.\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - Example #2\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tHypothesis: Would you call this \"How do it to be it?\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - Example #3\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - \tHypothesis: I call my name in Jam, and I don't have the Saa, but I was the rest of the United States.\n",
            "2021-07-08 10:29:05,800 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2500: bleu:   4.16, loss: 131674.3438, ppl:  46.9779, duration: 62.7247s\n",
            "2021-07-08 10:29:20,336 - INFO - joeynmt.training - Epoch   3, Step:     2600, Batch Loss:     2.995651, Tokens per Sec:    17550, Lr: 0.000300\n",
            "2021-07-08 10:29:34,947 - INFO - joeynmt.training - Epoch   3, Step:     2700, Batch Loss:     3.287580, Tokens per Sec:    17398, Lr: 0.000300\n",
            "2021-07-08 10:29:49,543 - INFO - joeynmt.training - Epoch   3, Step:     2800, Batch Loss:     3.295775, Tokens per Sec:    17273, Lr: 0.000300\n",
            "2021-07-08 10:30:04,113 - INFO - joeynmt.training - Epoch   3, Step:     2900, Batch Loss:     3.187446, Tokens per Sec:    17467, Lr: 0.000300\n",
            "2021-07-08 10:30:04,423 - INFO - joeynmt.training - Epoch   3: total training loss 3375.69\n",
            "2021-07-08 10:30:04,423 - INFO - joeynmt.training - Training ended after   3 epochs.\n",
            "2021-07-08 10:30:04,423 - INFO - joeynmt.training - Best validation result (greedy) at step     2500:  46.98 ppl.\n",
            "2021-07-08 10:30:04,442 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-07-08 10:30:04,598 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:30:04,788 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-08 10:30:04,848 - INFO - joeynmt.prediction - Decoding on dev set (/content/fr-en/parallel_dev.4000.bpe.en)...\n",
            "2021-07-08 10:30:46,588 - INFO - joeynmt.prediction -  dev bleu[intl]:   5.53 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-08 10:30:46,589 - INFO - joeynmt.prediction - Translations saved to: /content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en/00002500.hyps.dev\n",
            "2021-07-08 10:30:46,589 - INFO - joeynmt.prediction - Decoding on test set (/content/fr-en/parallel_test.4000.bpe.en)...\n",
            "2021-07-08 10:31:23,418 - INFO - joeynmt.prediction - test bleu[intl]:   5.08 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-08 10:31:23,419 - INFO - joeynmt.prediction - Translations saved to: /content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en/00002500.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3D6LbFGWV_7"
      },
      "source": [
        "If correct, the model should obtain roughly the following BLEU scores (or better!):\n",
        "\n",
        "\n",
        "*   Step 500: 0.05\n",
        "*   Step 1000: 1.24\n",
        "*   Step 1500: 1.97\n",
        "*   Step 2000: 3.71\n",
        "*   ...\n",
        "*   Step 3000: 7.12\n",
        "*   Step 8000: 15.30\n",
        "*   Step 17000: 21.69\n",
        "*   Step 27000: 23.67  (around 2h training time)\n",
        "\n",
        "You don't need to wait that long for the purpose of this exercise, Julia can provide a checkpoint for an already trained model :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8DIatjWHxQk"
      },
      "source": [
        "**TODO:**\n",
        "1. Why does the number of source words reported in the log not match the specified number of BPE merges? Tip: browse the [subword-nmt GitHub](https://github.com/rsennrich/subword-nmt).\n",
        "2. Imagine your job is to provide the best translation system as soon as possible. Try changing a few hyperparameters to see what the best score is that you can get within three epochs of training. You may also coordinate this with your colleagues.\n",
        "  * Suggestions: try changing bpe size, learning rate, batch size. \n",
        "  * Recommendation: create a new configuration and experiment directory for each experiment so you can tell them apart.\n",
        "  * You can spend endless time on this, but try to select a few settings that you'd hope could improve the result. \n",
        "  * Do you observe any tendency? Compare with your colleagues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxFq5xUbBZrt"
      },
      "source": [
        "*Notes*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3d3xwjVCJ3n"
      },
      "source": [
        "\n",
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQpIPDeKYpFA"
      },
      "source": [
        "For the following exercises you may either use your own model or the trained one provided by Julia (trained for 30 epochs).\n",
        "\n",
        "Now that we got a trained model, let's see how well it does. We'll probe for the following examples:\n",
        "\n",
        "1. A *training/memorization/overfitting* check: Did model learn to perfectly translate the training set?\n",
        "2. Unseen but from the *same domain*: Did the model learn to generalize to unseen examples?\n",
        "3. *Out-of-domain*: Can the model translate a random sentence from the source language?\n",
        "\n",
        "It will be increasingly hard for the model to do well on these. But even in the training set you can probably find outliers that the model does not translate well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDr8Fw3EONl"
      },
      "source": [
        "from joeynmt.helpers import load_config\n",
        "import yaml\n",
        "\n",
        "\n",
        "def download_pretrained_model_from_gdrive(\n",
        "    checkpoint='1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM',\n",
        "    config='1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV',\n",
        "    src_vocabulary='1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ',\n",
        "    trg_vocabulary='1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_',\n",
        "    bpe_merges='17XeygY048oXQHHzmH4u_hiJl1GndkiSN',\n",
        "    directory='/content/pretrained_model'):\n",
        "  \n",
        "  \"\"\"Download pretrained model from ids and place it in given directory. \n",
        "  Adjust paths in config as needed.\n",
        "  Default ids are for a model as specified above, but trained for the full\n",
        "  30 epochs. \n",
        "  \"\"\"\n",
        "\n",
        "  # Download files and place them into the new directory.\n",
        "  original_config = os.path.join(directory, 'original_config.yaml')\n",
        "  new_config = os.path.join(directory, 'config.yaml')\n",
        "  checkpoint_path = os.path.join(directory, 'best.ckpt')\n",
        "  trg_vocab_path = os.path.join(directory, 'trg_vocab.txt')\n",
        "  src_vocab_path = os.path.join(directory, 'src_vocab.txt')\n",
        "  bpe_path = os.path.join(directory, 'bpe.merges')\n",
        "\n",
        "  def gdown_by_id(id, output):\n",
        "    ! gdown 'https://drive.google.com/uc?id='$id -O $output\n",
        "\n",
        "  ! mkdir -p $directory\n",
        "  gdown_by_id(checkpoint, checkpoint_path)\n",
        "  gdown_by_id(config, original_config)\n",
        "  gdown_by_id(src_vocabulary, src_vocab_path)\n",
        "  gdown_by_id(trg_vocabulary, trg_vocab_path)\n",
        "  gdown_by_id(bpe_merges, bpe_path)\n",
        "\n",
        "  # Overwrite paths in config.\n",
        "  config = load_config(original_config)\n",
        "  config['data']['src_vocab'] = src_vocab_path\n",
        "  config['data']['trg_vocab'] = trg_vocab_path\n",
        "  config['training']['model_dir'] = directory\n",
        "  config['training']['load_model'] = checkpoint_path\n",
        "  with open(new_config, 'w') as cfile:\n",
        "    yaml.dump(config, cfile)\n",
        "  return new_config, bpe_path\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYaF8k1cDA9G",
        "outputId": "a1eca65b-3cc0-4532-de26-a4c78195ae13"
      },
      "source": [
        "# Download a pretrained model.\n",
        "pretrained_config, pretrained_bpe = download_pretrained_model_from_gdrive()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM\n",
            "To: /content/pretrained_model/best.ckpt\n",
            "157MB [00:01, 103MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV\n",
            "To: /content/pretrained_model/original_config.yaml\n",
            "100% 3.70k/3.70k [00:00<00:00, 4.74MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ\n",
            "To: /content/pretrained_model/src_vocab.txt\n",
            "100% 26.3k/26.3k [00:00<00:00, 3.51MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_\n",
            "To: /content/pretrained_model/trg_vocab.txt\n",
            "100% 26.3k/26.3k [00:00<00:00, 3.83MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17XeygY048oXQHHzmH4u_hiJl1GndkiSN\n",
            "To: /content/pretrained_model/bpe.merges\n",
            "100% 33.8k/33.8k [00:00<00:00, 29.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL6sJj4W6DoV"
      },
      "source": [
        "from subword_nmt import apply_bpe\n",
        "\n",
        "with open(bpe_file, \"r\") as merge_file:\n",
        "  bpe = apply_bpe.BPE(codes=merge_file)\n",
        "\n",
        "preprocess = lambda x: bpe.process_line(x.strip())"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5CgB0NP6IgI"
      },
      "source": [
        "my_sentence = 'Sidy marche lentement pour rejoindre fatou.'   # From https://eo.wikipedia.org/wiki/Esperanto"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIfSoDiP7T4a"
      },
      "source": [
        "my_sentence2='Merci beaucoup, Chris. C\\'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.'"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2Z4XjMT07778",
        "outputId": "ef007faa-79bc-4d7a-f002-a9b82e4edcff"
      },
      "source": [
        "preprocess(my_sentence2)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Merci beaucou@@ p, Chr@@ is. C'est vraiment un h@@ onn@@ eur de pouvoir venir sur cette sc@@ ène une deuxième fo@@ is. Je suis très re@@ connaiss@@ ant.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9KZ6HM-M6VoG",
        "outputId": "b0f68693-7b0e-43fc-fd63-64c91e5ad50f"
      },
      "source": [
        "preprocess(my_sentence)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'S@@ id@@ y mar@@ che l@@ ent@@ ement pour re@@ jo@@ indre f@@ at@@ ou@@ .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njMuETOe6eg5"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYRhVqjMH4RU"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "\n",
        "1.   Pick 2-5 sentences each from the three sets described above and translate them with your model in `translate` mode. Remember that you need to split them into BPEs first (already done for 1 and 2; example code for that in Lab 1).\n",
        "2.   Compare their translations: Can you tell from these examples what kind of data the model was trained on? Anything surprisingly good or bad?\n",
        "3. Choose one sentence that the model translated really well. Can you perturb it so that it's still very similar to the original but the translation is very different or significantly worse? \n",
        "\n",
        "Small changes in the input leading to small changes in the output can be seen as a criterion for robustness. The harder it is to find these adversarial inputs, the more robust is the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rIihpclD5tm",
        "outputId": "8948bc4b-f0bc-476b-cad6-781a82014d4e"
      },
      "source": [
        "# Either use $pretrained_config for the pretrained model or your own trained model.\n",
        "!python -m joeynmt translate $pretrained_config"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:31:31,612 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-08 10:31:35,314 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:31:35,506 - INFO - joeynmt.model - Enc-dec model built.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "S@@ id@@ y mar@@ che l@@ ent@@ ement pour re@@ jo@@ indre f@@ at@@ ou@@ .\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: Sidy works slowly to join fatous.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "\n",
            "Bye.\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.7/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1283, in _shutdown\n",
            "    if _main_thread._is_stopped:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQQrBC_b6q2g",
        "outputId": "8b4843c9-5975-4fff-9f72-49a4fbd32992"
      },
      "source": [
        "!python -m joeynmt translate $pretrained_config -n 5"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:39:29,403 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-08 10:39:33,136 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:39:33,331 - INFO - joeynmt.model - Enc-dec model built.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "S@@ id@@ y mar@@ che l@@ ent@@ ement pour re@@ jo@@ indre f@@ at@@ ou@@ .\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: Sidy works slowly to join fatous.\n",
            "JoeyNMT #2: Sidy works slowly to join satous.\n",
            "JoeyNMT #3: Sidy works slowly to join cratous.\n",
            "JoeyNMT #4: Sidy walks slowly to join fatous.\n",
            "JoeyNMT #5: Sidy works slowly to join fatou.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "\n",
            "Bye.\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErgpN-Ri-ifQ",
        "outputId": "ef185d66-d265-4385-ea24-a37c64555523"
      },
      "source": [
        "!python -m joeynmt translate $pretrained_config"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:39:42,022 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-08 10:39:45,709 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:39:45,904 - INFO - joeynmt.model - Enc-dec model built.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "S@@ id@@ y mar@@ che l@@ ent@@ ement pour re@@ jo@@ indre f@@ at@@ ou@@ .\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: Sidy works slowly to join fatous.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/prediction.py\", line 505, in translate\n",
            "    src_input = input(\"\\nPlease enter a source sentence \"\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/__main__.py\", line 42, in main\n",
            "    n_best=args.nbest)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joeynmt/prediction.py\", line 505, in translate\n",
            "    src_input = input(\"\\nPlease enter a source sentence \"\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_B8T6m77rJz",
        "outputId": "584f98b7-34f2-4618-a547-f28f3df32f6f"
      },
      "source": [
        "!python -m joeynmt translate $pretrained_config -n 5"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-08 10:39:54,224 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-08 10:39:57,937 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-08 10:39:58,134 - INFO - joeynmt.model - Enc-dec model built.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "S@@ id@@ y mar@@ che l@@ ent@@ ement pour re@@ jo@@ indre f@@ at@@ ou@@ .\n",
            "JoeyNMT: Hypotheses ranked by score\n",
            "JoeyNMT #1: Sidy works slowly to join fatous.\n",
            "JoeyNMT #2: Sidy works slowly to join satous.\n",
            "JoeyNMT #3: Sidy works slowly to join cratous.\n",
            "JoeyNMT #4: Sidy walks slowly to join fatous.\n",
            "JoeyNMT #5: Sidy works slowly to join fatou.\n",
            "\n",
            "Please enter a source sentence (pre-processed): \n",
            "\n",
            "Bye.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2B83tnyDopt"
      },
      "source": [
        "*Notes:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1yq7rq0I1UF"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG1UC2rfKewE"
      },
      "source": [
        "We now got an intuition of what the model can do and where its limits are. During validation, we trust the BLEU score to tell us whether the model is progressing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPc2LjGK3-S"
      },
      "source": [
        "1.   Compute the `sacrebleu` ([GitHub](https://github.com/mjpost/sacrebleu)) score for the dev set translations for a chosen step that are stored in your model directory (`.hyps`). Below is an example call. Do they match with the result that was reported in `validations.txt` and `train.log`?\n",
        "2.   In the configuration we chose one particular tokenizer, but there are other options (hint: explore sacrebleu documentation). Does the reported score change? If so, why do you think this happens?\n",
        "3. The `sacrebleu` library also implements the ChrF score. Compute the ChrF as well as the BLEU score for two validation steps. How do differ with respect to ChrF and BLEU, are the differences comparable?\n",
        "\n",
        "(We did not tokenize our data before feeding it to the model. Do you think it makes a difference? You can try it out with the `sacremoses` library that implements tokenizers.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHkNRNIrUHVP"
      },
      "source": [
        "# Helper function\n",
        "def read_sentences(inputfile):\n",
        "  \"\"\"Read sentences from file into list.\"\"\"\n",
        "  lines = []\n",
        "  with open(inputfile, 'r') as ofile:\n",
        "    for line in ofile:\n",
        "      lines.append(line.strip())\n",
        "  print(f'Read {len(lines)} sentences from file {inputfile}.')\n",
        "  return lines"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13-fIRnUogy",
        "outputId": "b5311121-9edb-4f70-feae-359a896a61de"
      },
      "source": [
        "# Model outputs\n",
        "hyps = read_sentences('/content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en/1000.hyps')\n",
        "#hyps=read_sentences('/content/drive/MyDrive/NMT_Lab2/models/fr-en/ted_fr_en/train.log')\n",
        "# And references for the same dev set\n",
        "refs = read_sentences('/content/fr-en/parallel_dev.en')\n",
        "# refs = read_sentences('')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1306 sentences from file /content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en/1000.hyps.\n",
            "Read 1306 sentences from file /content/fr-en/parallel_dev.en.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsdJUZdYLbqH"
      },
      "source": [
        "import sacrebleu"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9q9lYJnRwFY",
        "outputId": "3a692468-886a-4c64-a271-b57f7bbf5d84"
      },
      "source": [
        "sacrebleu.corpus_bleu(hyps, [refs])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 0.72 13.8/1.4/0.3/0.0 (BP = 1.000 ratio = 1.328 hyp_len = 32447 ref_len = 24429)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk_0TAeRU-29"
      },
      "source": [
        "Note the one-element list that we're passing to the BLEU score calculation. This is because BLEU was originally proposed to compute quality scores relative to multiple translations. However, in practice there are rarely multiple translations available, so we got to work with what we have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCzlKnCZKs1j"
      },
      "source": [
        "# Extra: Backtranslation & Multilingual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VhSYBwNRcVM"
      },
      "source": [
        "These experiments take more time than you'll have in the lab and relate to contents covered later this week. They might be interesting to explore if you want to keep learning about NMT :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX9_V4veK7fM"
      },
      "source": [
        "### Backtranslation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRq7DZ4FYhgE"
      },
      "source": [
        "The downloaded data also contains a `train.en` file: monolingual data for English. This can be used to improve our model with backtranslation.  There are multiple steps and options involved:\n",
        "  * First, you need to train a en-fr model.\n",
        "  * Then use this reverse model to translate this monolingual data (or a part thereof, depending on translation speed).\n",
        "  * Now you have synthetic training data that you can either 1) mix with the original training data as it is, 2) mix with a certain ratio, since this data has probably lower quality. \n",
        "  * You can then either 1) further train the original `fr-en` model on this data, or 2) retrain a `fr-en` model to see if it gets better than the original data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McyAT7BPYidz"
      },
      "source": [
        "*Notes:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q7Do7l1K_JF"
      },
      "source": [
        "### Multilingual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jsgyXYDLCuG"
      },
      "source": [
        "The downloaded directory also contains for other languages paired with English on the target side: `ar`, `de`, `ja`, `ko`, `zh`. Additional training data from other languages often helps to improve translation quality for small training data. \n",
        "\n",
        "We'll try out the \"many-to-one\" approach here: learning to translate from many languages into English. For the opposite, we would need to add special target language tags to the source (.e.g. `<2fr>`, `<2ja>` to tell the model which language it should translate into.\n",
        "\n",
        "* First, select one or more language pairs to add to fr-en.\n",
        "* Repeat the pre-processing pipeline for them. Training and dev sets should get concatenated for joint training. BPE training should also be done on a concatenation of the training sets for all languages, so that the sub-word merges reflect all languages.\n",
        "* Depending on the number of languages, your concatenated dev set might grow too large for regular validation during training, so you can also just take a smaller subset from each language and combine them. \n",
        "* Do you find improvements over the original model? For a direct comparison you would need to translate the original fr-en dev or test set with the multilingual model (not the concatenated ones used for this experiment) and compare the scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deylviR3RZGI"
      },
      "source": [
        "*Notes:*"
      ]
    }
  ]
}